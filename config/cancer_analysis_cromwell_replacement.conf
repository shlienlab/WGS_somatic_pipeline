include required(classpath("application"))
backend {
  default = singularity
  providers {
    singularity {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        job-shell = "/bin/bash"
        run-in-background = true
        filesystems {
          local {
            localization: ["soft-link", "hard-link", "copy"]
          }
        }

        runtime-attributes = """
            String? docker
            String? image
            Int memory_gb
            Int? cpu = 1
            String walltime
            String log_dir
            String task_name
            String? output_dir
        """

        submit = """
        qsub -m ae \
        -N ${task_name} \
        -V \
        -S /bin/bash \
        -o ${log_dir} \
        -e ${log_dir} \
        -W umask=0117 \
        -l mem=${memory_gb}g,walltime=${walltime},nodes=1:ppn=${cpu} \
        -N ${task_name} \
        ${script}
        """

        submit-docker = """
            unset PERL5LIB

            # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
            # based on the users home.
            export SINGULARITY_CACHEDIR=PATH_TO_YOUR_SINGULARITY_CACHE
            
            echo ${memory_gb}
            my_uuid=`uuidgen -r`
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
            IMAGE_STORE=PATH_TO_YOUR_LOCAL_IMAGE_STORE
            IMAGE=$IMAGE_STORE/$DOCKER_NAME.sif
            HPF=${output_dir}

            s_script=${cwd}/"singularity_script_"$my_uuid".sh"
            echo "#!/bin/bash" >> $s_script
            #echo ". ~/.bashrc" >> $s_script
            echo "cd $PWD" >> $s_script
            echo "mkdir ${cwd}/var" >> $s_script
            echo "mkdir ${cwd}/var/tmp" >> $s_script
            echo "SCRATCH=/localhd/\$PBS_JOBID"  >> $s_script
            echo "$SCRATCH"
            echo "module load Singularity/3.5.3" >> $s_script
            #echo "echo in s_script" >> $s_script
            #echo "env" >> $s_script
            echo "singularity exec --cleanenv --bind \$SCRATCH:/scratch,$HPF,${cwd}/var/tmp:/var/tmp,${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}" >> $s_script

            qsub -m ae \
            -N ${task_name} \
            -V \
            -S /bin/bash \
            -o ${log_dir} \
            -e ${log_dir} \
            -W umask=0117 \
            -l mem=${memory_gb}g,walltime=${walltime},nodes=1:ppn=${cpu},file=2000gb \
            $s_script
        """

        job-id-regex = "(\\d+)"

        kill = "qdel ${job_id}"
        check-alive = "qstat ${job_id}"
      }
    }

  }
}

docker {
  hash-lookup {
    # Set this to match your available quota against the Google Container Engine API
    #gcr-api-queries-per-100-seconds = 1000

    # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    #cache-entry-ttl = "20 minutes"

    # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    #cache-size = 200

    # How should docker hashes be looked up. Possible values are "local" and "remote"
    # "local": Lookup hashes on the local docker daemon using the cli
    # "remote": Lookup hashes on docker hub and gcr
    method = "local"
  }
}

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    url = """
    jdbc:hsqldb:file:REPLACE_ME/cromwell-db/cromwell-db;
    shutdown=false;
    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
    hsqldb.result_max_memory_rows=10000;
    hsqldb.large_data=true;
    hsqldb.applog=1;
    hsqldb.lob_compressed=true;
    hsqldb.script_format=3
    """
    connectionTimeout = 600000
    numThreads = 1
   }
}

workflow-options {
  workflow-failure-mode = "NoNewCalls"
}
